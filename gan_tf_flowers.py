# -*- coding: utf-8 -*-
"""GAN_tf_flowers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12IKnAV1M1iqny3WkQpxDZuamG4wCzTZZ
"""

#-- Install Required Libraries -------------------------------------------------
!pip install numpy2tfrecord
!pip install Albumentations
!pip install tensorboardX
#-------------------------------------------------------------------------------

#-- Imports --------------------------------------------------------------------
import tensorflow as tf
import tensorflow_datasets as tfds

from keras import layers
from keras import optimizers 
from keras.models import Model , Sequential
from keras.initializers import RandomNormal
from keras.utils import plot_model
from keras.callbacks import TensorBoard

from tensorboardX import SummaryWriter

import albumentations as A

from numpy2tfrecord import Numpy2TFRecordConverter

from functools import partial

import numpy as np
import random
import datetime

import matplotlib.pyplot as plt
from IPython.display import Image
#-------------------------------------------------------------------------------

#-- Initialising Parameters ----------------------------------------------------

np.random.seed(10)  
NOISE_VEC_DIM = 100  

IMG_ROW, IMG_COL, IMG_CHANNEL = 64, 64, 3

BATCH_SIZE = 128
STEPS_PER_EPOCH = 50
EPOCHS = 150

OPTIMIZER = optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)

FILTER_SIZE = 5
N_FILTERS= 64
DROPOUT_PROB = 0.5
SCALE = 0.2
INPUT_SIZE = [IMG_ROW, IMG_COL, IMG_CHANNEL]
#-------------------------------------------------------------------------------

#-- Load DS --------------------------------------------------------------------
ds, ds_info = tfds.load(
    'tf_flowers',    
    with_info=True,
    as_supervised=True,
)

training_set = ds['train']
#-------------------------------------------------------------------------------

#-- Plot a few Frist Images ----------------------------------------------------
for image, label in training_set.take(5):
  image = image.numpy()
  plt.figure()
  plt.title(int(label))  
  plt.imshow(image, cmap=plt.cm.binary)
  plt.show()
#-------------------------------------------------------------------------------

#-- Extract Images from DS -----------------------------------------------------
images = []
for image, label in training_set.take(-1):
  images.append(image.numpy())

print(len(images))
#-------------------------------------------------------------------------------

#-- Image Augmentation ---------------------------------------------------------
#-- list of augmentations --
transform = A.Compose([
    A.RandomRotate90(),
    A.Transpose(),
    A.ShiftScaleRotate(shift_limit=0.08, scale_limit=0.5, rotate_limit=5, p=.8),
    A.Blur(blur_limit=7),
    A.GridDistortion(),
])

#-- Run a Maximum of 3 Augmentations on each Image --
augmented_images = images.copy()
max_aug = 3
for img in images:  
  n_aug = random.randint(1, max_aug)
  for i in range(n_aug):
    aug_img = transform(image=img)['image']
    augmented_images.append(aug_img)

print(len(augmented_images))
#-------------------------------------------------------------------------------

#-- Show Images Size -----------------------------------------------------------
for i in range(5):
  print('Image {} shape: {} '.format(i+1, images[i].shape))
#-------------------------------------------------------------------------------

#-- Resize Image ---------------------------------------------------------------
def Resize_Image(image):
  image = tf.image.resize(image, (IMG_ROW, IMG_COL))/255.0
  return image
#-------------------------------------------------------------------------------

#-- Resize All Images --
training_images = list(map(Resize_Image, augmented_images))

#-- Show Images Size --
for i in range(5):
  print('Image {} shape: {} '.format(i+1, training_images[i].shape))
#-------------------------------------------------------------------------------

#-- Save Images as tfrecord ----------------------------------------------------
#-- Set path and file name --
path_tfrecords = 'images.tfrecords'

#-- convert all images to tfrecords --
samples = []
with Numpy2TFRecordConverter(path_tfrecords) as converter:
  for img in training_images:
    img = img.numpy()
    img = img.astype(np.float32)
    sample = {"img": img}
    samples .append(sample)
    
  converter.convert_list(samples)
#-------------------------------------------------------------------------------

#-- Read each tfrecord and decode it to an image -------------------------------
def Read_tfrecord(sample):
    tfrecord_format = (
         {'img': tf.io.FixedLenFeature([IMG_ROW,IMG_COL,IMG_CHANNEL], tf.float32),}
    )
    sample = tf.io.parse_single_example(sample, tfrecord_format)   
    image = sample['img']   
    
    return image
#-------------------------------------------------------------------------------

#-- Load DS from tfrecords file ------------------------------------------------
def Load_Dataset(filename):
    ignore_order = tf.data.Options()
    ignore_order.experimental_deterministic = False  
    
    dataset = tf.data.TFRecordDataset(filename)  
    dataset = dataset.with_options(ignore_order)    
    dataset = dataset.map(partial(Read_tfrecord))  
    
    return dataset
#-------------------------------------------------------------------------------

#-- Load DS from tfrecords and Save as Numpy Array -----------------------------
dataset = Load_Dataset(path_tfrecords)
print(type(dataset))

ds_size = len(list(dataset))

X_train  = np.zeros((ds_size,IMG_ROW,IMG_COL,IMG_CHANNEL))

index = 0
for image in dataset.take(-1):  
  X_train[index] = image
  index += 1

np.random.shuffle(X_train)
print(X_train.shape)
#-------------------------------------------------------------------------------

#-- Generator 1 ----------------------------------------------------------------
def Create_Generator_1():

    generator = Sequential()        
    #------------------------------------------------------
    
    d = 4
    generator.add(layers.Dense(d*d*512,
                               input_shape=(NOISE_VEC_DIM,),
                               name='gen_Dense')) 
    print(generator.input_shape)    
    #------------------------------------------------------

    generator.add(layers.Reshape((d, d, 512),
                                 name='gen_Reshape'))
    print(generator.output_shape)
    #------------------------------------------------------ 

    generator.add(layers.Conv2DTranspose(4*N_FILTERS,
                                         (FILTER_SIZE, FILTER_SIZE),
                                         name='gen_Con2DTrans_1'))   
    generator.add(layers.BatchNormalization(name='gen_BatchNorm_1'))
    generator.add(layers.LeakyReLU(name='gen_LRelU_1'))
    print(generator.output_shape)
    #------------------------------------------------------  

    generator.add(layers.Conv2DTranspose(2*N_FILTERS,
                                         (FILTER_SIZE, FILTER_SIZE),
                                         strides=(2, 2),
                                         padding='same',
                                         name='gen_Con2DTrans_2'))  
    generator.add(layers.BatchNormalization(name='gen_BatchNorm_2'))
    generator.add(layers.LeakyReLU(name='gen_LRelU_2'))
    print(generator.output_shape)
    #------------------------------------------------------  

    generator.add(layers.Conv2DTranspose(N_FILTERS,
                                         (FILTER_SIZE, FILTER_SIZE),
                                         strides=(2, 2),
                                         padding='same',
                                         name='gen_Con2DTrans_3'))   
    generator.add(layers.BatchNormalization(name='gen_BatchNorm_3'))
    generator.add(layers.LeakyReLU(name='gen_LRelU_3'))
    print(generator.output_shape)

    #------------------------------------------------------

    generator.add(layers.Conv2DTranspose(3,
                                         (FILTER_SIZE, FILTER_SIZE),
                                         strides=(2, 2),
                                         padding='same',
                                         activation='tanh',
                                         name='gen_Con2DTrans_4'))  
    print(generator.output_shape)
    #------------------------------------------------------  
        
    generator.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)      
    #------------------------------------------------------
    
    return generator
#-------------------------------------------------------------------------------

#-- Discriminator 1 ------------------------------------------------------------
def Create_Discriminator_1():
    discriminator = Sequential()    
    #-----------------------------------------------------------

    discriminator.add(layers.InputLayer(input_shape=INPUT_SIZE,
                                        name = 'disc_Input'))
    discriminator.add(layers.Dropout(DROPOUT_PROB,
                                     name= 'disc_Dropout_1'))
    print(discriminator.output_shape)
    #-----------------------------------------------------------

    discriminator.add(layers.Conv2D(N_FILTERS,
                            (FILTER_SIZE, FILTER_SIZE),
                            strides=(2, 2),
                            padding='same',
                            name='disc_Conv_1'))
    discriminator.add(layers.LeakyReLU(SCALE,
                                       name = 'disc_LRelU_1'))    
    print(discriminator.output_shape)
    #-----------------------------------------------------------

    discriminator.add(layers.Conv2D(2*N_FILTERS,
                            (FILTER_SIZE, FILTER_SIZE),
                            strides=(2, 2),
                            padding='same',
                            name='disc_Conv_2'))
    discriminator.add(layers.BatchNormalization(name='BatchNorm_2'))
    discriminator.add(layers.LeakyReLU(SCALE,
                                       name = 'disc_LRelU_2'))
    print(discriminator.output_shape)
    #-----------------------------------------------------------

    discriminator.add(layers.Conv2D(4*N_FILTERS,
                            (FILTER_SIZE, FILTER_SIZE),
                            strides=(2, 2),
                            padding='same',
                            name='disc_Conv_3'))
    discriminator.add(layers.BatchNormalization(name='BatchNorm_3'))
    discriminator.add(layers.LeakyReLU(SCALE,
                                       name = 'disc_LRelU_3'))
    print(discriminator.output_shape)
    #-----------------------------------------------------------

    discriminator.add(layers.Conv2D(8*N_FILTERS,
                            (FILTER_SIZE, FILTER_SIZE),
                            strides=(2, 2),
                            padding='same',
                            name='disc_Conv_4'))
    discriminator.add(layers.BatchNormalization(name='BatchNorm_4'))
    discriminator.add(layers.LeakyReLU(SCALE,
                                       name = 'disc_LRelU_4'))
    print(discriminator.output_shape)
    #-----------------------------------------------------------

    discriminator.add(layers.Flatten(name='disc_Flatten'))
    discriminator.add(layers.Dropout(0.4, name='disc_Dropout'))
    
    discriminator.add(layers.Dense(1,
                                   activation='sigmoid',
                                   input_shape=(IMG_COL, IMG_ROW, IMG_CHANNEL),
                                   name='disc_Output_2'))
    #-----------------------------------------------------------
    
    discriminator.compile(loss='binary_crossentropy', optimizer=OPTIMIZER) 
    #----------------------------------------------------------- 
    
    return discriminator
#-------------------------------------------------------------------------------

#-- Generator 2 ----------------------------------------------------------------
def Create_Generator_2():
    generator = Sequential()    
    #----------------------------------------------------

    d = 16
    generator.add(layers.Dense(d*d*256,
                               kernel_initializer=RandomNormal(0, 0.02),
                               input_dim=NOISE_VEC_DIM,
                               name = 'gen_dense'))
    generator.add(layers.LeakyReLU(0.2,
                                   name = 'gen_LRelU_0'))   
    print(generator.input_shape) 
    #----------------------------------------------------

    generator.add(layers.Reshape((d, d, 256),
                                 name='gen_Reshape'))
    print(generator.output_shape)
    #----------------------------------------------------
    
    generator.add(layers.Conv2DTranspose(128, (4, 4),
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=RandomNormal(0, 0.02),
                                         name = 'gen_Con2DTrans_1'))
    generator.add(layers.LeakyReLU(0.2,
                                   name = 'gen_LRelU_1'))
    print(generator.output_shape)
    #----------------------------------------------------

    generator.add(layers.Conv2DTranspose(128, (4, 4),
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=RandomNormal(0, 0.02),
                                         name = 'gen_Con2DTrans_2'))
    generator.add(layers.LeakyReLU(0.2,
                                   name = 'gen_LRelU_2'))
    print(generator.output_shape)
    #----------------------------------------------------
    
    generator.add(layers.Conv2D(IMG_CHANNEL, (3, 3),
                                padding='same',
                                activation='tanh',
                                kernel_initializer=RandomNormal(0, 0.02),
                                name = 'gen_Con2DTrans_3')) 
    print(generator.output_shape)
    #----------------------------------------------------

    generator.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)      
    #---------------------------------------------------- 
    
    return generator
#-------------------------------------------------------------------------------

#-- Discriminator 2 ------------------------------------------------------------
def Create_Discriminator_2():

    discriminator = Sequential()
    #----------------------------------------------------
    
    discriminator.add(layers.Conv2D(64,
                                    (3, 3),
                                    padding='same',
                                    kernel_initializer=RandomNormal(0, 0.02),
                                    input_shape=(IMG_COL, IMG_ROW, IMG_CHANNEL),
                                    name= 'disc_Conv_1'))
    discriminator.add(layers.LeakyReLU(0.2,
                                       name='disc_LRelU_1'))
    print(discriminator.output_shape)
    #----------------------------------------------------
    
    discriminator.add(layers.Conv2D(128,
                                    (3, 3),
                                    strides=2,
                                    padding='same',
                                    kernel_initializer=RandomNormal(0, 0.02),
                                    name= 'disc_Conv_2'))
    discriminator.add(layers.LeakyReLU(0.2,
                                       name='disc_LRelU_2'))
    print(discriminator.output_shape)
    #----------------------------------------------------
    
    discriminator.add(layers.Conv2D(128,
                                    (3, 3),
                                    strides=2,
                                    padding='same',
                                    kernel_initializer=RandomNormal(0, 0.02),
                                    name= 'disc_Conv_3'))
    discriminator.add(layers.LeakyReLU(0.2,
                                       name='disc_LRelU_3'))
    print(discriminator.output_shape)
    #----------------------------------------------------
    
    discriminator.add(layers.Conv2D(256,
                                    (3, 3),
                                    strides=2,
                                    padding='same',
                                    kernel_initializer=RandomNormal(0, 0.02),
                                    name= 'disc_Conv_4'))
    discriminator.add(layers.LeakyReLU(0.2,
                                       name='disc_LRelU_4'))
    print(discriminator.output_shape)
    #----------------------------------------------------
    
    discriminator.add(layers.Flatten(name='disc_Flatten'))
    discriminator.add(layers.Dropout(0.4,
                                     name = 'disc_Dropout'))
    print(discriminator.output_shape)
    #----------------------------------------------------
    
    discriminator.add(layers.Dense(1,
                                   activation='sigmoid',
                                   input_shape=(IMG_COL, IMG_ROW, IMG_CHANNEL),
                                   name = 'disc_Output'))
    print(discriminator.output_shape)
    #----------------------------------------------------
    
    discriminator.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)  
    #----------------------------------------------------

    return discriminator

#-- Create GAN-----------------------------------------------------------------
def Create_GAN(discriminator , generator):           

    discriminator.trainable = False          

    gan_input = layers.Input(shape=(NOISE_VEC_DIM,), name='gan_Input')    
    fake_image = generator(gan_input)        

    gan_output = discriminator(fake_image)   

    gan = Model(gan_input, gan_output)       
    
    gan.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)   
    
    return gan
#-------------------------------------------------------------------------------

#-- Plot Imgages----------------------------------------------------------------
def Plot_Images(noise, size_fig, generator, epoch):

    #-- get generated images by generator --
    generated_images = generator.predict(noise)   

    #-- create a figure --
    plt.figure(figsize=size_fig)
    
    for i, image in enumerate(generated_images):
        plt.subplot(size_fig[0], size_fig[1], i+1)
        if IMG_CHANNEL == 1:
            image = image.reshape((IMG_ROW, IMG_COL))
            plt.imshow(image, cmap='gray')    
        else:
            image = image.reshape((IMG_ROW, IMG_COL, IMG_CHANNEL))
            plt.imshow(image)
        
        Write_Log_Images(image , epoch)

        plt.axis('off')        
    
    plt.tight_layout()   
    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch+1))
    plt.show()
#-------------------------------------------------------------------------------

#-- Plot Losses-----------------------------------------------------------------
def Plot_Loss(d_losses, g_losses):
    
    plt.rcParams.update({'font.size': 22})
    
    d_losses = np.round(d_losses , 3)
    g_losses = np.round(g_losses , 3)
    
    x_plot = range(1, len(d_losses)+1)
    
    plt.figure(figsize=(7,4))    
    
    plt.plot(x_plot, d_losses , label= 'discriminator')
    
    plt.plot(x_plot , g_losses, label = 'generator')
    
    
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.grid()
    plt.show()
#-------------------------------------------------------------------------------

#-- Create GAN Model 1 ---------------------------------------------------------

#-- Create generator --
generator = Create_Generator_1()

#-- Models Summery--generator --
plot_model(generator, show_shapes=True,
                       show_layer_names=True, to_file='generator.png')
Image(retina=True, filename='generator.png')
generator.summary()

#-- Create discriminator --
discriminator = Create_Discriminator_1()

#-- Models Summery--discriminator --
plot_model(discriminator, show_shapes=True,
                       show_layer_names=True, to_file='discriminator.png')
Image(retina=True, filename='discriminator.png')
discriminator.summary()

#-- Create GAN --
gan = Create_GAN(discriminator, generator)

#-- Models Summery--gan --
plot_model(gan, show_shapes=True,
                       show_layer_names=True, to_file='gan.png')
Image(retina=True, filename='gan.png')
gan.summary()
#-------------------------------------------------------------------------------

#-- Create GAN Model 2  --------------------------------------------------------
#-- Create generator --
generator = Create_Generator_2()

#-- Models Summery--generator --
plot_model(generator, show_shapes=True,
                       show_layer_names=True, to_file='generator.png')
Image(retina=True, filename='generator.png')
generator.summary()

#-- Create discriminator --
discriminator = Create_Discriminator_2()

#-- Models Summery--discriminator --
plot_model(discriminator, show_shapes=True,
                       show_layer_names=True, to_file='discriminator.png')
Image(retina=True, filename='discriminator.png')
discriminator.summary()

#-- Create GAN --
gan = Create_GAN(discriminator, generator)

#-- Models Summery--gan --
plot_model(gan, show_shapes=True,
                       show_layer_names=True, to_file='gan.png')
Image(retina=True, filename='gan.png')
gan.summary()
#-------------------------------------------------------------------------------

# Commented out IPython magic to ensure Python compatibility.
#-- Tensorboard ----------------------------------------------------------------
#-- load tensorboard --
# %load_ext tensorboard

log_path = '/tmp/my_logs'

#-- Clear any logs from previous runs --
!rm -rf ./tmp/my_logs 

#-- Create the TensorBoard --
tensorboard = TensorBoard(
  log_dir= log_path,
  histogram_freq=0,  
  write_graph=True,  
)

#-- set gan for tensroboard model --
tensorboard.set_model(gan)
#-------------------------------------------------------------------------------

#-- Write Losses in Log file for Tensorboard -----------------------------------
def Write_Log_Loss(names, logs, batch_no):
  writer = SummaryWriter()
  for name, value in zip(names, logs):
    writer.add_scalar(name, value, batch_no)
    writer.flush()
#-------------------------------------------------------------------------------

#-- Save images in Log file for Tensorboard ------------------------------------
def Write_Log_Images(image , epoch):
  writer = SummaryWriter()     
  writer.add_image('fake_image',image,epoch+1, dataformats='HWC')
  writer.close()
#-------------------------------------------------------------------------------

#-- Delete Not Required Variables ----------------------------------------------

del ds
del ds_info
del training_set
del images
del augmented_images
del training_images
del samples
del dataset

import gc
gc.collect()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /tmp/my_logs

#-- Train ----------------------------------------------------------------------
d_losses = []
g_losses = []

STEPS_PER_EPOCH = int(X_train.shape[0]/BATCH_SIZE)+1
print(STEPS_PER_EPOCH)



for epoch in range(EPOCHS):
  for batch in range(STEPS_PER_EPOCH):
    
    #-- Create random vector --
    noise = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_VEC_DIM))

    #-- Generate fake images by generator --
    fake_x = generator.predict(noise)

    #-- Get real images
    real_x = X_train[np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)]
    
    #-- Concat real images and fake images --
    x = np.concatenate((real_x, fake_x))
    
    #-- set true output for real images and fake images --
    disc_y = np.zeros(2*BATCH_SIZE)
    disc_y[:BATCH_SIZE] = 0.9

    #-- train discriminator --
    d_loss = discriminator.train_on_batch(x, disc_y)

    #-- set true output for generator --
    y_gen = np.ones(BATCH_SIZE)

    #-- train gan --
    g_loss = gan.train_on_batch(noise, y_gen)

    #-- Save Logs for tensorboard on each batch--    
    if batch==0 or (batch+1)%10==0:
      Write_Log_Loss(['g_loss'], [g_loss], batch)
      Write_Log_Loss(['d_loss'], [d_loss], batch)

  #-- save losses for each epoch--
  d_losses.append(d_loss)
  g_losses.append(g_loss)
    
  #-- Print losses for each epoch
  print(f'Epoch: {epoch + 1} \t Discriminator Loss: {d_loss} \t\t Generator Loss: {g_loss}')

  #-- Plot Generead Images by Generator for each epoch --
  if epoch==0 or (epoch+1)%10==0:
    noise = np.random.normal(0, 1, size=(25, NOISE_VEC_DIM))
    Plot_Images(noise, (5, 5), generator, epoch) 

  #-- Save weights for each epoch --
  if epoch==0 or (epoch+1)%100==0:
    weights_file = 'gan_weights_{:04d}.h5'.format(epoch+1)
    gan.save_weights(weights_file)

  gc.collect()

Plot_Loss(d_losses, g_losses)